{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-world",
   "metadata": {},
   "source": [
    "# Introducción Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-american",
   "metadata": {},
   "source": [
    "![elgifderigor](https://media.giphy.com/media/NsBknNwmmWE8WU1q2U/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-vault",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Algunos-conceptos\" data-toc-modified-id=\"Algunos-conceptos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Algunos conceptos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimadores\" data-toc-modified-id=\"Estimadores-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Estimadores</a></span></li><li><span><a href=\"#Error-de-Bias\" data-toc-modified-id=\"Error-de-Bias-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Error de Bias (Sesgo)</a></span></li><li><span><a href=\"#Error-de-Varianza\" data-toc-modified-id=\"Error-de-Varianza-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Error de Varianza</a></span></li></ul></li><li><span><a href=\"#Tipos-de-Aprendizaje-Automático\" data-toc-modified-id=\"Tipos-de-Aprendizaje-Automático-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tipos de Aprendizaje Automático</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Underfitting-y-Overfitting\" data-toc-modified-id=\"Underfitting-y-Overfitting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Underfitting y Overfitting</a></span></li><li><span><a href=\"#¿Manos-a-la-obra?\" data-toc-modified-id=\"¿Manos-a-la-obra?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>¿Manos a la obra?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Practicamos-ML-con-un-ejemplo-de-regresión\" data-toc-modified-id=\"Practicamos-ML-con-un-ejemplo-de-regresión-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Practicamos ML con un ejemplo de regresión</a></span></li><li><span><a href=\"#Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)\" data-toc-modified-id=\"Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Cargamos un dataset (aunque ya nos resulta famliar)</a></span></li><li><span><a href=\"#Train-/-Test-Split-¿Esto-qué-es?\" data-toc-modified-id=\"Train-/-Test-Split-¿Esto-qué-es?-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Train / Test Split ¿Esto qué es?</a></span></li><li><span><a href=\"#Preparación-de-los-datos-para-train-test-split\" data-toc-modified-id=\"Preparación-de-los-datos-para-train-test-split-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Preparación de los datos para train test split</a></span></li><li><span><a href=\"#Parámetros-opcionales-del-train-test-split\" data-toc-modified-id=\"Parámetros-opcionales-del-train-test-split-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Parámetros opcionales del train test split</a></span></li><li><span><a href=\"#Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo\" data-toc-modified-id=\"Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo</a></span></li><li><span><a href=\"#Creamos-nuestras-predicciones\" data-toc-modified-id=\"Creamos-nuestras-predicciones-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Creamos nuestras predicciones</a></span></li><li><span><a href=\"#Medimos-nuestro-modelo\" data-toc-modified-id=\"Medimos-nuestro-modelo-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Medimos nuestro modelo</a></span></li></ul></li><li><span><a href=\"#Extra!-Entrenando-diferentes-modelos-a-la-vez\" data-toc-modified-id=\"Extra!-Entrenando-diferentes-modelos-a-la-vez-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Extra! Entrenando diferentes modelos a la vez</a></span><ul class=\"toc-item\"><li><span><a href=\"#Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario\" data-toc-modified-id=\"Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Almacenamos todos los modelos que vamos a entrenar en un diccionario</a></span></li><li><span><a href=\"#Iteramos-sobre-los-modelos-para-entrenarlos\" data-toc-modified-id=\"Iteramos-sobre-los-modelos-para-entrenarlos-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Iteramos sobre los modelos para entrenarlos</a></span></li></ul></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cross-Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hacemos-CV-a-un-solo-modelo\" data-toc-modified-id=\"Hacemos-CV-a-un-solo-modelo-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Hacemos CV a un solo modelo</a></span></li><li><span><a href=\"#CV-iterando-por-todos-los-modelos\" data-toc-modified-id=\"CV-iterando-por-todos-los-modelos-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>CV iterando por todos los modelos</a></span></li></ul></li><li><span><a href=\"#Summmary\" data-toc-modified-id=\"Summmary-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Summmary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-pharmaceutical",
   "metadata": {},
   "source": [
    "## Algunos conceptos\n",
    "Para entender cómo un algoritmo de aprendizaje automático aprende de los datos para predecir un resultado, es esencial comprender los conceptos subyacentes que intervienen en el entrenamiento de un algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-aquatic",
   "metadata": {},
   "source": [
    "### Estimadores\n",
    "La estimación es un término estadístico para encontrar alguna estimación de un parámetro desconocido, dados algunos datos. La estimación puntual es el intento de proporcionar la mejor predicción de alguna cantidad de interés.\n",
    "La cantidad de interés puede ser: \n",
    " - Un único parámetro\n",
    " - Un vector de parámetros - por ejemplo, los pesos en la regresión lineal \n",
    " - Una función completa\n",
    "\n",
    "### Error de Bias (Sesgo)\n",
    "\n",
    "Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos.\n",
    "\n",
    "Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como ajuste insuficiente.\n",
    "\n",
    "En general, los algoritmos paramétricos como la regresión lineal, tienen un alto bias que los hace rápidos de aprender y más fácil de entender, pero generalmente menos flexibles. A su vez, tienen un menor rendimiento predictivo en problemas complejos.\n",
    "\n",
    "### Error de Varianza\n",
    "\n",
    "Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro, lo que significa que el algoritmo es bueno para elegir el mapeo subyacente oculto entre las variables de entrada y de salida.\n",
    "\n",
    "Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo.\n",
    "\n",
    "Generalmente, los algoritmos de Machine Learning no paramétrico que tienen mucha flexibilidad tienen una gran variación.\n",
    "\n",
    "- Varianza baja: sugiere pequeños cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "- Alta varianza: sugiere grandes cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "Los algoritmos de Machine Learning con baja varianza incluyen: regresión lineal, análisis discriminante lineal y regresión logística.\n",
    "\n",
    "Por su parte, los algoritmos con alta varianza son: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9c76",
   "metadata": {},
   "source": [
    "<img src=\"https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-intro",
   "metadata": {},
   "source": [
    "## Tipos de Aprendizaje Automático\n",
    "**- Supervisado**      \n",
    "El modelo se entrena con datos etiquetados, es decir, con datos cuya verdad básica se conoce. Cuando se le presentan datos y una etiqueta, el modelo infiere patrones.      \n",
    "**- No supervisado**          \n",
    "No hay verdades básicas. El modelo busca patrones no detectados previamente, con los que separar los distintos puntos de datos en diferentes clusters.\n",
    "**- Refuerzo**    \n",
    "Tampoco existe una verdad de base. La acción del modelo se valora y se da una recompensa o un castigo en consecuencia. El objetivo del modelo es obtener el mayor número de recompensas posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-bobby",
   "metadata": {},
   "source": [
    "## Features\n",
    "Las **Features** son variables individuales independientes que actúan como entrada en su sistema. Los modelos de predicción utilizan características para\n",
    "hacer predicciones.      \n",
    "**Objetivo**        \n",
    "El objetivo es la salida de las variables de entrada. Pueden ser las clases individuales a las que se asignan las variables de entrada en el caso de un problema de clasificación o el rango de valores de salida en un problema de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-nelson",
   "metadata": {},
   "source": [
    "## Underfitting y Overfitting  \n",
    "Dividir un conjunto de datos también puede ser importante para detectar si su modelo sufre uno de los dos problemas más comunes, denominados infraajuste y sobreajuste:\n",
    "\n",
    "El **underfitting** suele ser la consecuencia de que un modelo sea incapaz de encapsular las relaciones entre los datos. Por ejemplo, esto puede ocurrir cuando se intenta representar relaciones no lineales con un modelo lineal. Los modelos infraajustados probablemente tendrán un rendimiento pobre tanto en los conjuntos de entrenamiento como en los de prueba.\n",
    "\n",
    "El **overfitting** suele tener lugar cuando un modelo tiene una estructura excesivamente compleja y aprende tanto las relaciones existentes entre los datos como el ruido. Estos modelos suelen tener una mala capacidad de generalización. Aunque funcionan bien con los datos de entrenamiento, suelen tener un rendimiento pobre con los datos no vistos (de prueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42891daf",
   "metadata": {},
   "source": [
    "<img src=\"https://community.alteryx.com/t5/image/serverpage/image-id/52874iE986B6E19F3248CF?v=v2\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-apparel",
   "metadata": {},
   "source": [
    "## ¿Manos a la obra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-jackson",
   "metadata": {},
   "source": [
    "### Practicamos ML con un ejemplo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legitimate-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-reconstruction",
   "metadata": {},
   "source": [
    "### Cargamos un dataset (aunque ya nos resulta famliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29aba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_boston in module sklearn.datasets._base:\n",
      "\n",
      "load_boston(*, return_X_y=False)\n",
      "    Load and return the boston house-prices dataset (regression).\n",
      "    \n",
      "    ==============   ==============\n",
      "    Samples total               506\n",
      "    Dimensionality               13\n",
      "    Features         real, positive\n",
      "    Targets           real 5. - 50.\n",
      "    ==============   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : ndarray of shape (506, 13)\n",
      "            The data matrix.\n",
      "        target : ndarray of shape (506, )\n",
      "            The regression target.\n",
      "        filename : str\n",
      "            The physical location of boston csv dataset.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        feature_names : ndarray\n",
      "            The names of features\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed a wrong data point at [445, 0].\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_boston\n",
      "    >>> X, y = load_boston(return_X_y=True)\n",
      "    >>> print(X.shape)\n",
      "    (506, 13)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_boston()['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-elite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston=pd.DataFrame(load_boston().data, columns=load_boston().feature_names)\n",
    "\n",
    "boston['MEDV']=load_boston().target\n",
    "\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-bacteria",
   "metadata": {},
   "source": [
    "### Train / Test Split ¿Esto qué es?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e15f7",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-grove",
   "metadata": {},
   "source": [
    "La división entrenamiento-prueba es una técnica para evaluar el rendimiento de un algoritmo de aprendizaje automático.\n",
    "\n",
    "Puede utilizarse para problemas de clasificación o regresión y puede emplearse para cualquier algoritmo de aprendizaje supervisado.\n",
    "\n",
    "El procedimiento consiste en tomar un conjunto de datos y dividirlo en dos subconjuntos. El primer subconjunto se utiliza para ajustar el modelo y se denomina conjunto de datos de entrenamiento. El segundo subconjunto no se utiliza para entrenar el modelo; en su lugar, se proporciona al modelo el elemento de entrada del conjunto de datos, luego se hacen predicciones y se comparan con los valores esperados. Este segundo conjunto de datos se denomina conjunto de datos de prueba.\n",
    "\n",
    "Conjunto de datos de entrenamiento: Se utiliza para ajustar el modelo de aprendizaje automático.\n",
    "Conjunto de datos de prueba: Se utiliza para evaluar el modelo de aprendizaje automático ajustado.\n",
    "El objetivo es estimar el rendimiento del modelo de aprendizaje automático con datos nuevos: datos no utilizados para entrenar el modelo.\n",
    "\n",
    "Así es como esperamos utilizar el modelo en la práctica. Es decir, ajustarlo sobre los datos disponibles con entradas y salidas conocidas, para luego hacer predicciones sobre nuevos ejemplos en el futuro en los que no tenemos los valores de salida u objetivo esperados.\n",
    "\n",
    "El procedimiento de entrenamiento-prueba es adecuado cuando se dispone de un conjunto de datos suficientemente amplio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-texas",
   "metadata": {},
   "source": [
    "### Preparación de los datos para train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automatic-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT', 'MEDV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seasonal-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=boston.drop('MEDV', axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sealed-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=boston.MEDV\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closed-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intensive-newark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-musician",
   "metadata": {},
   "source": [
    "### Parámetros opcionales del train test split\n",
    "\n",
    "**train_size** es el número que define el tamaño del conjunto de entrenamiento. Si proporciona un float, entonces debe estar entre 0.0 y 1.0 y definirá la parte del conjunto de datos utilizado para las pruebas. Si proporciona un int, entonces representará el número total de las muestras de entrenamiento. El valor por defecto es None.\n",
    "\n",
    "**test_size** es el número que define el tamaño del conjunto de pruebas. Es muy similar a train_size. Debe proporcionar o bien train_size o bien test_size. Si no se proporciona ninguno de los dos, la parte por defecto del conjunto de datos que se utilizará para las pruebas es 0,25, o el 25 por ciento.\n",
    "\n",
    "**random_state** es el objeto que controla la aleatoriedad durante la división. Puede ser un int o una instancia de RandomState. El valor por defecto es None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "private-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4aeb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState instance, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hybrid-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flying-alpha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comic-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "destroyed-velvet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "digital-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-defense",
   "metadata": {},
   "source": [
    "### Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ultimate-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "\n",
    "linreg=LinReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15fb5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a7c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion(*args):\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69ccfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 4, 5, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "funcion(3, 3, 4, 5, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0196c3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module sklearn.linear_model._base object:\n",
      "\n",
      "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      " |  LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      " |  \n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      " |  to minimize the residual sum of squares between the observed targets in\n",
      " |  the dataset, and the targets predicted by the linear approximation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
      " |      an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This will only provide\n",
      " |      speedup for n_targets > 1 and sufficient large problems.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  rank_ : int\n",
      " |      Rank of matrix `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  singular_ : array of shape (min(X, y),)\n",
      " |      Singular values of `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  intercept_ : float or array of shape (n_targets,)\n",
      " |      Independent term in the linear model. Set to 0.0 if\n",
      " |      `fit_intercept = False`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
      " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      " |      size of the coefficients with l2 regularization.\n",
      " |  sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
      " |      sparse coefficients with l1 regularization.\n",
      " |  sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
      " |      model trained with both l1 and l2 -norm regularization of the\n",
      " |      coefficients.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LinearRegression\n",
      " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      " |  >>> reg = LinearRegression().fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  1.0\n",
      " |  >>> reg.coef_\n",
      " |  array([1., 2.])\n",
      " |  >>> reg.intercept_\n",
      " |  3.0000...\n",
      " |  >>> reg.predict(np.array([[3, 5]]))\n",
      " |  array([16.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "small-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "929b5ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.04101801e-01,  4.46607962e-02,  1.47975466e-02,  2.12950789e+00,\n",
       "       -1.70202488e+01,  3.74872759e+00, -2.72114100e-03, -1.51129780e+00,\n",
       "        3.05960473e-01, -1.28097230e-02, -9.72068358e-01,  9.64014122e-03,\n",
       "       -5.16032791e-01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e239a19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.19855220843748"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-migration",
   "metadata": {},
   "source": [
    "### Creamos nuestras predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "executed-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "single-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.63062051, 24.5005557 , 13.71216264, 25.03316715, 14.22425052,\n",
       "       22.42972766, 13.83880621, 20.20222564, 31.87425281, 25.22986847])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21e670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      16.5\n",
       "100    27.5\n",
       "408    17.2\n",
       "347    23.1\n",
       "428    11.0\n",
       "270    21.1\n",
       "138    13.3\n",
       "393    13.8\n",
       "369    50.0\n",
       "375    15.0\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68033dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cc7e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-imaging",
   "metadata": {},
   "source": [
    "### Medimos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "treated-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ordered-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MAE - Error Medio Absoluto', 3.0041718431725455)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'MAE - Error Medio Absoluto', metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f77afc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MSE - Error Cuadratico Medio', 20.24827014789769)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'MSE - Error Cuadratico Medio', metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f41fde1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RMSE - Raiz Error Cuadratico Medio', 4.499807790105894)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'RMSE - Raiz Error Cuadratico Medio', metrics.mean_squared_error(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49e194be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('R2 - Coeficiente de Determinacion', 0.7136297223659916)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'R2 - Coeficiente de Determinacion', metrics.r2_score(y_test, y_pred)  # para el testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2d56983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el entrenamiento\n",
    "\n",
    "y_pred_train=linreg.predict(X_train)   # prediccion con datos conocidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2324989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7423712239243331"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(y_train, y_pred_train)   # R2 para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b4c6150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.703315985520815"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(y_train, y_pred_train)**0.5  # RMSE para entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-banking",
   "metadata": {},
   "source": [
    "## Extra! Entrenando diferentes modelos a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "designed-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5768ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingRegressor in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingRegressor(sklearn.base.RegressorMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingRegressor(*, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for regression.\n",
      " |  \n",
      " |  GB builds an additive model in a forward stage-wise fashion;\n",
      " |  it allows for the optimization of arbitrary differentiable loss functions.\n",
      " |  In each stage a regression tree is fit on the negative gradient of the\n",
      " |  given loss function.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n",
      " |      loss function to be optimized. 'ls' refers to least squares\n",
      " |      regression. 'lad' (least absolute deviation) is a highly robust\n",
      " |      loss function solely based on order information of the input\n",
      " |      variables. 'huber' is a combination of the two. 'quantile'\n",
      " |      allows quantile regression (use `alpha` to specify the quantile).\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"friedman_mse\" for the mean squared error with improvement\n",
      " |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      " |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
      " |      initial raw predictions are set to zero. By default a\n",
      " |      ``DummyEstimator`` is used, predicting either the average target value\n",
      " |      (for loss='ls'), or a quantile for the other losses.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random spliting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  alpha : float, default=0.9\n",
      " |      The alpha-quantile of the huber loss function and the quantile\n",
      " |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  presort : deprecated, default='deprecated'\n",
      " |      This parameter is deprecated and will be removed in v0.24.\n",
      " |  \n",
      " |      .. deprecated :: 0.22\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> from sklearn.ensemble import GradientBoostingRegressor\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_regression(random_state=0)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      " |  ...     X, y, random_state=0)\n",
      " |  >>> reg = GradientBoostingRegressor(random_state=0)\n",
      " |  >>> reg.fit(X_train, y_train)\n",
      " |  GradientBoostingRegressor(random_state=0)\n",
      " |  >>> reg.predict(X_test[1:2])\n",
      " |  array([-61...])\n",
      " |  >>> reg.score(X_test, y_test)\n",
      " |  0.4...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.ensemble.HistGradientBoostingRegressor,\n",
      " |  sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict regression target at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-momentum",
   "metadata": {},
   "source": [
    "### Almacenamos todos los modelos que vamos a entrenar en un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "former-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(),\n",
    "    'sgd': SGDRegressor(),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'grad': GradientBoostingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-pierce",
   "metadata": {},
   "source": [
    "### Iteramos sobre los modelos para entrenarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7052b4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ridge', Ridge())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(models.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rising-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando:  ridge\n",
      "Entrenando:  lasso\n",
      "Entrenando:  sgd\n",
      "Entrenando:  knn\n",
      "Entrenando:  grad\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():  # entrenamiento en bucle\n",
    "    print('Entrenando: ', name)\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bigger-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo :  ridge\n",
      "RMSE - Raiz Error Cuadratico Medio 4.563889212183413\n",
      "R2 - Coeficiente de Determinacion 0.7054152904619431\n",
      "RMSE_train - Raiz Error Cuadratico Medio 4.719898843186611\n",
      "R2_train - Coeficiente de Determinacion 0.7405513365113647\n",
      "\n",
      "Modelo :  lasso\n",
      "RMSE - Raiz Error Cuadratico Medio 5.250726971707366\n",
      "R2 - Coeficiente de Determinacion 0.6100769698644182\n",
      "RMSE_train - Raiz Error Cuadratico Medio 5.166657120291662\n",
      "R2_train - Coeficiente de Determinacion 0.6891110128557125\n",
      "\n",
      "Modelo :  sgd\n",
      "RMSE - Raiz Error Cuadratico Medio 215580678548050.84\n",
      "R2 - Coeficiente de Determinacion -6.57294023145621e+26\n",
      "RMSE_train - Raiz Error Cuadratico Medio 220225147615946.5\n",
      "R2_train - Coeficiente de Determinacion -5.648327572139386e+26\n",
      "\n",
      "Modelo :  knn\n",
      "RMSE - Raiz Error Cuadratico Medio 6.936912349249951\n",
      "R2 - Coeficiente de Determinacion 0.3194305844861024\n",
      "RMSE_train - Raiz Error Cuadratico Medio 4.899945009783385\n",
      "R2_train - Coeficiente de Determinacion 0.7203798475071796\n",
      "\n",
      "Modelo :  grad\n",
      "RMSE - Raiz Error Cuadratico Medio 2.630950307753527\n",
      "R2 - Coeficiente de Determinacion 0.9021039193297228\n",
      "RMSE_train - Raiz Error Cuadratico Medio 1.4141188034075909\n",
      "R2_train - Coeficiente de Determinacion 0.9767106241274044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    \n",
    "    print('Modelo : ', name)\n",
    "    \n",
    "    y_pred=model.predict(X_test)\n",
    "    \n",
    "    y_pred_train=model.predict(X_train)\n",
    "    \n",
    "    print('RMSE - Raiz Error Cuadratico Medio', metrics.mean_squared_error(y_test, y_pred)**0.5)\n",
    "    print('R2 - Coeficiente de Determinacion', metrics.r2_score(y_test, y_pred))\n",
    "    \n",
    "    print('RMSE_train - Raiz Error Cuadratico Medio', metrics.mean_squared_error(y_train, y_pred_train)**0.5)\n",
    "    print('R2_train - Coeficiente de Determinacion', metrics.r2_score(y_train, y_pred_train))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-mention",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Leamos un poco la [documentación](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "**Wikipedia nos dice**            \n",
    "La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. Consiste en repetir y calcular la media aritmética obtenida de las medidas de evaluación sobre diferentes particiones. Se utiliza en entornos donde el objetivo principal es la predicción y se quiere estimar la precisión de un modelo que se llevará a cabo a la práctica. Es una técnica muy utilizada en proyectos de inteligencia artificial para validar modelos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cellular-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las métricas para el cross validation son el R2 para regresión y el Accuracy para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-walnut",
   "metadata": {},
   "source": [
    "![kfoldcv](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-affiliation",
   "metadata": {},
   "source": [
    "Cross-Validation: K-fold con 5 splits\n",
    "Lo que hacemos normalmente al entrenar el modelo es pasarle todos los registros y que haga el fit().      \n",
    "Con K-Folds -en este ejemplo de 5 splits- para entrenar, en vez de pasarle todos los registros directamente al modelo, haremos así:\n",
    "\n",
    "**Iterar 5 veces:**      \n",
    "- Apartaremos 1/5 de muestras\n",
    "- Entrenamos al modelo con el restante 4/5 de muestras\n",
    "- Mediremos el r2 obtenido sobre las que habíamos apartado.\n",
    "- Esto quiere decir que hacemos 5 entrenamientos independientes.\n",
    "- El R2 final será el promedio de las 5 R2 anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-reservation",
   "metadata": {},
   "source": [
    "### Hacemos CV a un solo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "joined-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03486994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "beneficial-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=cvs(models['grad'], X, y, scoring='r2', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prostate-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78316128, 0.85274558, 0.76119214, 0.57155124, 0.42962163])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "afraid-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796543735831135"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0137e039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['grad'].fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-harvey",
   "metadata": {},
   "source": [
    "### CV iterando por todos los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "surprised-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo:  ridge Score:  0.3892175824102393\n",
      "Modelo:  lasso Score:  0.431848787926522\n",
      "Modelo:  sgd Score:  -3.122912341809478e+26\n",
      "Modelo:  knn Score:  -0.31501646812514134\n",
      "Modelo:  grad Score:  0.6674736000503803\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    scores=cvs(model, X, y, scoring='r2', cv=5)\n",
    "    print('Modelo: ', name, 'Score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-being",
   "metadata": {},
   "source": [
    "## Summmary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-opera",
   "metadata": {},
   "source": [
    "- Dividir dataset en entrenamiento / testeo para poder sacar métricas\n",
    "- La y es nuestra variable target que es la que queremos predecir\n",
    "- Siempre limpiamos y hacemos el preprocesado de los datos, tomamos decisiones con las filas que tengan valores nulos\n",
    "- Hay diferentes modelos de regresión, que se aproximan al mismo problema con diferentes fórmulas matemáticas\n",
    "- Dos grandes problemas -> clasificación y regresión. \n",
    "- Clasificación multiclass / multilabel "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
