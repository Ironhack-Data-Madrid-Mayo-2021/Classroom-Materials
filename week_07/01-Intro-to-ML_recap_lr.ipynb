{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-world",
   "metadata": {},
   "source": [
    "# Introducción Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-american",
   "metadata": {},
   "source": [
    "![elgifderigor](https://media.giphy.com/media/NsBknNwmmWE8WU1q2U/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-vault",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Algunos-conceptos\" data-toc-modified-id=\"Algunos-conceptos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Algunos conceptos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimadores\" data-toc-modified-id=\"Estimadores-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Estimadores</a></span></li><li><span><a href=\"#Error-de-Bias\" data-toc-modified-id=\"Error-de-Bias-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Error de Bias (Sesgo)</a></span></li><li><span><a href=\"#Error-de-Varianza\" data-toc-modified-id=\"Error-de-Varianza-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Error de Varianza</a></span></li></ul></li><li><span><a href=\"#Tipos-de-Aprendizaje-Automático\" data-toc-modified-id=\"Tipos-de-Aprendizaje-Automático-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tipos de Aprendizaje Automático</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Underfitting-y-Overfitting\" data-toc-modified-id=\"Underfitting-y-Overfitting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Underfitting y Overfitting</a></span></li><li><span><a href=\"#¿Manos-a-la-obra?\" data-toc-modified-id=\"¿Manos-a-la-obra?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>¿Manos a la obra?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Practicamos-ML-con-un-ejemplo-de-regresión\" data-toc-modified-id=\"Practicamos-ML-con-un-ejemplo-de-regresión-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Practicamos ML con un ejemplo de regresión</a></span></li><li><span><a href=\"#Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)\" data-toc-modified-id=\"Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Cargamos un dataset (aunque ya nos resulta famliar)</a></span></li><li><span><a href=\"#Train-/-Test-Split-¿Esto-qué-es?\" data-toc-modified-id=\"Train-/-Test-Split-¿Esto-qué-es?-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Train / Test Split ¿Esto qué es?</a></span></li><li><span><a href=\"#Preparación-de-los-datos-para-train-test-split\" data-toc-modified-id=\"Preparación-de-los-datos-para-train-test-split-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Preparación de los datos para train test split</a></span></li><li><span><a href=\"#Parámetros-opcionales-del-train-test-split\" data-toc-modified-id=\"Parámetros-opcionales-del-train-test-split-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Parámetros opcionales del train test split</a></span></li><li><span><a href=\"#Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo\" data-toc-modified-id=\"Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo</a></span></li><li><span><a href=\"#Creamos-nuestras-predicciones\" data-toc-modified-id=\"Creamos-nuestras-predicciones-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Creamos nuestras predicciones</a></span></li><li><span><a href=\"#Medimos-nuestro-modelo\" data-toc-modified-id=\"Medimos-nuestro-modelo-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Medimos nuestro modelo</a></span></li></ul></li><li><span><a href=\"#Extra!-Entrenando-diferentes-modelos-a-la-vez\" data-toc-modified-id=\"Extra!-Entrenando-diferentes-modelos-a-la-vez-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Extra! Entrenando diferentes modelos a la vez</a></span><ul class=\"toc-item\"><li><span><a href=\"#Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario\" data-toc-modified-id=\"Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Almacenamos todos los modelos que vamos a entrenar en un diccionario</a></span></li><li><span><a href=\"#Iteramos-sobre-los-modelos-para-entrenarlos\" data-toc-modified-id=\"Iteramos-sobre-los-modelos-para-entrenarlos-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Iteramos sobre los modelos para entrenarlos</a></span></li></ul></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cross-Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hacemos-CV-a-un-solo-modelo\" data-toc-modified-id=\"Hacemos-CV-a-un-solo-modelo-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Hacemos CV a un solo modelo</a></span></li><li><span><a href=\"#CV-iterando-por-todos-los-modelos\" data-toc-modified-id=\"CV-iterando-por-todos-los-modelos-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>CV iterando por todos los modelos</a></span></li></ul></li><li><span><a href=\"#Summmary\" data-toc-modified-id=\"Summmary-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Summmary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-pharmaceutical",
   "metadata": {},
   "source": [
    "## Algunos conceptos\n",
    "Para entender cómo un algoritmo de aprendizaje automático aprende de los datos para predecir un resultado, es esencial comprender los conceptos subyacentes que intervienen en el entrenamiento de un algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-aquatic",
   "metadata": {},
   "source": [
    "### Estimadores\n",
    "La estimación es un término estadístico para encontrar alguna estimación de un parámetro desconocido, dados algunos datos. La estimación puntual es el intento de proporcionar la mejor predicción de alguna cantidad de interés.\n",
    "La cantidad de interés puede ser: \n",
    " - Un único parámetro\n",
    " - Un vector de parámetros - por ejemplo, los pesos en la regresión lineal \n",
    " - Una función completa\n",
    "\n",
    "### Error de Bias (Sesgo)\n",
    "\n",
    "Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos.\n",
    "\n",
    "Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como ajuste insuficiente.\n",
    "\n",
    "En general, los algoritmos paramétricos como la regresión lineal, tienen un alto bias que los hace rápidos de aprender y más fácil de entender, pero generalmente menos flexibles. A su vez, tienen un menor rendimiento predictivo en problemas complejos.\n",
    "\n",
    "### Error de Varianza\n",
    "\n",
    "Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro, lo que significa que el algoritmo es bueno para elegir el mapeo subyacente oculto entre las variables de entrada y de salida.\n",
    "\n",
    "Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo.\n",
    "\n",
    "Generalmente, los algoritmos de Machine Learning no paramétrico que tienen mucha flexibilidad tienen una gran variación.\n",
    "\n",
    "- Varianza baja: sugiere pequeños cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "- Alta varianza: sugiere grandes cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "Los algoritmos de Machine Learning con baja varianza incluyen: regresión lineal, análisis discriminante lineal y regresión logística.\n",
    "\n",
    "Por su parte, los algoritmos con alta varianza son: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9c76",
   "metadata": {},
   "source": [
    "<img src=\"https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-intro",
   "metadata": {},
   "source": [
    "## Tipos de Aprendizaje Automático\n",
    "**- Supervisado**      \n",
    "El modelo se entrena con datos etiquetados, es decir, con datos cuya verdad básica se conoce. Cuando se le presentan datos y una etiqueta, el modelo infiere patrones.      \n",
    "**- No supervisado**          \n",
    "No hay verdades básicas. El modelo busca patrones no detectados previamente, con los que separar los distintos puntos de datos en diferentes clusters.\n",
    "**- Refuerzo**    \n",
    "Tampoco existe una verdad de base. La acción del modelo se valora y se da una recompensa o un castigo en consecuencia. El objetivo del modelo es obtener el mayor número de recompensas posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-bobby",
   "metadata": {},
   "source": [
    "## Features\n",
    "Las **Features** son variables individuales independientes que actúan como entrada en su sistema. Los modelos de predicción utilizan características para\n",
    "hacer predicciones.      \n",
    "**Objetivo**        \n",
    "El objetivo es la salida de las variables de entrada. Pueden ser las clases individuales a las que se asignan las variables de entrada en el caso de un problema de clasificación o el rango de valores de salida en un problema de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-nelson",
   "metadata": {},
   "source": [
    "## Underfitting y Overfitting  \n",
    "Dividir un conjunto de datos también puede ser importante para detectar si su modelo sufre uno de los dos problemas más comunes, denominados infraajuste y sobreajuste:\n",
    "\n",
    "El **underfitting** suele ser la consecuencia de que un modelo sea incapaz de encapsular las relaciones entre los datos. Por ejemplo, esto puede ocurrir cuando se intenta representar relaciones no lineales con un modelo lineal. Los modelos infraajustados probablemente tendrán un rendimiento pobre tanto en los conjuntos de entrenamiento como en los de prueba.\n",
    "\n",
    "El **overfitting** suele tener lugar cuando un modelo tiene una estructura excesivamente compleja y aprende tanto las relaciones existentes entre los datos como el ruido. Estos modelos suelen tener una mala capacidad de generalización. Aunque funcionan bien con los datos de entrenamiento, suelen tener un rendimiento pobre con los datos no vistos (de prueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42891daf",
   "metadata": {},
   "source": [
    "<img src=\"https://community.alteryx.com/t5/image/serverpage/image-id/52874iE986B6E19F3248CF?v=v2\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-apparel",
   "metadata": {},
   "source": [
    "## ¿Manos a la obra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-jackson",
   "metadata": {},
   "source": [
    "### Practicamos ML con un ejemplo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legitimate-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-reconstruction",
   "metadata": {},
   "source": [
    "### Cargamos un dataset (aunque ya nos resulta famliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29aba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_boston in module sklearn.datasets._base:\n",
      "\n",
      "load_boston(*, return_X_y=False)\n",
      "    Load and return the boston house-prices dataset (regression).\n",
      "    \n",
      "    ==============   ==============\n",
      "    Samples total               506\n",
      "    Dimensionality               13\n",
      "    Features         real, positive\n",
      "    Targets           real 5. - 50.\n",
      "    ==============   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : ndarray of shape (506, 13)\n",
      "            The data matrix.\n",
      "        target : ndarray of shape (506, )\n",
      "            The regression target.\n",
      "        filename : str\n",
      "            The physical location of boston csv dataset.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        feature_names : ndarray\n",
      "            The names of features\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed a wrong data point at [445, 0].\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_boston\n",
      "    >>> X, y = load_boston(return_X_y=True)\n",
      "    >>> print(X.shape)\n",
      "    (506, 13)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b461bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_boston()['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-elite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = load_boston()\n",
    "boston = pd.DataFrame(boston_data.data, columns = boston_data.feature_names)\n",
    "boston[\"MEDV\"] = boston_data.target\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-bacteria",
   "metadata": {},
   "source": [
    "### Train / Test Split ¿Esto qué es?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e15f7",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-grove",
   "metadata": {},
   "source": [
    "La división entrenamiento-prueba es una técnica para evaluar el rendimiento de un algoritmo de aprendizaje automático.\n",
    "\n",
    "Puede utilizarse para problemas de clasificación o regresión y puede emplearse para cualquier algoritmo de aprendizaje supervisado.\n",
    "\n",
    "El procedimiento consiste en tomar un conjunto de datos y dividirlo en dos subconjuntos. El primer subconjunto se utiliza para ajustar el modelo y se denomina conjunto de datos de entrenamiento. El segundo subconjunto no se utiliza para entrenar el modelo; en su lugar, se proporciona al modelo el elemento de entrada del conjunto de datos, luego se hacen predicciones y se comparan con los valores esperados. Este segundo conjunto de datos se denomina conjunto de datos de prueba.\n",
    "\n",
    "Conjunto de datos de entrenamiento: Se utiliza para ajustar el modelo de aprendizaje automático.\n",
    "Conjunto de datos de prueba: Se utiliza para evaluar el modelo de aprendizaje automático ajustado.\n",
    "El objetivo es estimar el rendimiento del modelo de aprendizaje automático con datos nuevos: datos no utilizados para entrenar el modelo.\n",
    "\n",
    "Así es como esperamos utilizar el modelo en la práctica. Es decir, ajustarlo sobre los datos disponibles con entradas y salidas conocidas, para luego hacer predicciones sobre nuevos ejemplos en el futuro en los que no tenemos los valores de salida u objetivo esperados.\n",
    "\n",
    "El procedimiento de entrenamiento-prueba es adecuado cuando se dispone de un conjunto de datos suficientemente amplio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-texas",
   "metadata": {},
   "source": [
    "### Preparación de los datos para train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automatic-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT', 'MEDV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seasonal-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = boston.drop('MEDV', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sealed-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = boston.MEDV\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closed-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intensive-newark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-musician",
   "metadata": {},
   "source": [
    "### Parámetros opcionales del train test split\n",
    "\n",
    "**train_size** es el número que define el tamaño del conjunto de entrenamiento. Si proporciona un float, entonces debe estar entre 0.0 y 1.0 y definirá la parte del conjunto de datos utilizado para las pruebas. Si proporciona un int, entonces representará el número total de las muestras de entrenamiento. El valor por defecto es None.\n",
    "\n",
    "**test_size** es el número que define el tamaño del conjunto de pruebas. Es muy similar a train_size. Debe proporcionar o bien train_size o bien test_size. Si no se proporciona ninguno de los dos, la parte por defecto del conjunto de datos que se utilizará para las pruebas es 0,25, o el 25 por ciento.\n",
    "\n",
    "**random_state** es el objeto que controla la aleatoriedad durante la división. Puede ser un int o una instancia de RandomState. El valor por defecto es None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "private-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4aeb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hybrid-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flying-alpha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comic-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "destroyed-velvet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "digital-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-defense",
   "metadata": {},
   "source": [
    "### Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ultimate-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el objeto LinearRegression en una variable\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "small-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-migration",
   "metadata": {},
   "source": [
    "### Creamos nuestras predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "executed-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "single-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.12513043, 33.61333501, 20.43957462, 15.5425527 , 22.37196803,\n",
       "       24.03156356, 20.23026241, 12.18965537, 33.35118517, 16.70908423,\n",
       "       41.33620036, 16.5917617 , 23.79974254, 33.16975583, 15.82828278,\n",
       "       22.66883076, 21.17739233, 30.19396864, 32.17389104, 37.39315691,\n",
       "       12.07881724, 20.69649817, 20.70771275, 16.86371553, 22.38338836,\n",
       "       20.52649415, 28.79702161, 31.72702259, 25.07363644, 25.12011538,\n",
       "       24.90761336, 27.96510317, 19.24014655, 19.97988441, 10.33403601,\n",
       "       25.83507717, 15.81538154, 35.8116483 , 16.37312558, 13.13552288,\n",
       "       20.27309379, 30.84073345, 20.30518837, 31.15102383, 23.2328606 ,\n",
       "       12.76478106,  7.24735433, 21.72670028, 17.66814385, 14.11298416,\n",
       "       15.6055013 , 16.38462245, 13.3371744 , 17.9134525 , 12.66567704,\n",
       "       13.87163222, 21.76856357, 34.04904696, 31.02466447, 20.98460571,\n",
       "       15.93686501, 28.87605315, 35.975308  , 16.79382602, 37.40997185,\n",
       "       41.68026871, 20.8394717 , 26.71998636, 20.5897291 , 22.7394218 ,\n",
       "       24.52047297, 28.35678651, 17.31308919, 25.89144647, 25.17638201,\n",
       "       16.05546496, 20.18214417, 20.91065005, 36.12260211, 39.23129839,\n",
       "       27.00028201, 10.3278501 , 26.27235323, 19.96937773, 24.7878525 ,\n",
       "       14.31908401, 27.59090265, 16.7559092 , 36.96462112, 23.64884264,\n",
       "       21.65539409, 23.06128062, 23.74973339, 22.49029637, 11.46795169,\n",
       "       24.97789169, 16.18570399, 22.11724149, 30.87850285, 21.47150627,\n",
       "       21.11914096, 17.6763111 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-imaging",
   "metadata": {},
   "source": [
    "### Medimos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "treated-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ordered-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3.439931377595603\n",
      "MSE:  30.346207394397393\n",
      "RMSE:  5.508739183733188\n",
      "R2:  0.6238398727076448\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE: \", metrics.mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "print(\"R2: \", metrics.r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-banking",
   "metadata": {},
   "source": [
    "## Extra! Entrenando diferentes modelos a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "designed-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-momentum",
   "metadata": {},
   "source": [
    "### Almacenamos todos los modelos que vamos a entrenar en un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "former-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = { \"ridge\": Ridge(),\n",
    "          \"lasso\": Lasso(),\n",
    "          \"sgd\": SGDRegressor(),\n",
    "          \"knn\": KNeighborsRegressor(),\n",
    "          \"gradient\": GradientBoostingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-pierce",
   "metadata": {},
   "source": [
    "### Iteramos sobre los modelos para entrenarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rising-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo ---> ridge\n",
      "He acabado :)\n",
      "Entrenando modelo ---> lasso\n",
      "He acabado :)\n",
      "Entrenando modelo ---> sgd\n",
      "He acabado :)\n",
      "Entrenando modelo ---> knn\n",
      "He acabado :)\n",
      "Entrenando modelo ---> gradient\n",
      "He acabado :)\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Entrenando modelo ---> {name}\")\n",
    "    model.fit(X_train,y_train)\n",
    "    print(f\"He acabado :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bigger-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------ridge--------\n",
      "MAE:  3.3704126433260906\n",
      "MSE:  30.15102562544698\n",
      "RMSE:  5.490994957696372\n",
      "R2:  0.626259272209511\n",
      "\n",
      "\n",
      "--------lasso--------\n",
      "MAE:  3.6514918455828287\n",
      "MSE:  29.0189771906401\n",
      "RMSE:  5.386926506890557\n",
      "R2:  0.6402917171145269\n",
      "\n",
      "\n",
      "--------sgd--------\n",
      "MAE:  38285409892055.7\n",
      "MSE:  2.3453813555950174e+27\n",
      "RMSE:  48429137464908.64\n",
      "R2:  -2.9072461602981723e+25\n",
      "\n",
      "\n",
      "--------knn--------\n",
      "MAE:  4.1647058823529415\n",
      "MSE:  35.974392156862756\n",
      "RMSE:  5.997865633445181\n",
      "R2:  0.5540750197506085\n",
      "\n",
      "\n",
      "--------gradient--------\n",
      "MAE:  2.1675902508009295\n",
      "MSE:  14.443036605840495\n",
      "RMSE:  3.8003995323966264\n",
      "R2:  0.8209695723247399\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Podemos seguir el mismo proceso para realizar predicciones de cada algoritmo y sacar métricas\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"--------{name}--------\")\n",
    "    print(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print(\"MSE: \", metrics.mean_squared_error(y_test,y_pred))\n",
    "    print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "    print(\"R2: \", metrics.r2_score(y_test,y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-mention",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Leamos un poco la [documentación](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "**Wikipedia nos dice**            \n",
    "La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. Consiste en repetir y calcular la media aritmética obtenida de las medidas de evaluación sobre diferentes particiones. Se utiliza en entornos donde el objetivo principal es la predicción y se quiere estimar la precisión de un modelo que se llevará a cabo a la práctica. Es una técnica muy utilizada en proyectos de inteligencia artificial para validar modelos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cellular-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las métricas para el cross validation son el R2 para regresión y el Accuracy para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-walnut",
   "metadata": {},
   "source": [
    "![kfoldcv](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-affiliation",
   "metadata": {},
   "source": [
    "Cross-Validation: K-fold con 5 splits\n",
    "Lo que hacemos normalmente al entrenar el modelo es pasarle todos los registros y que haga el fit().      \n",
    "Con K-Folds -en este ejemplo de 5 splits- para entrenar, en vez de pasarle todos los registros directamente al modelo, haremos así:\n",
    "\n",
    "**Iterar 5 veces:**      \n",
    "- Apartaremos 1/5 de muestras\n",
    "- Entrenamos al modelo con el restante 4/5 de muestras\n",
    "- Mediremos el r2 obtenido sobre las que habíamos apartado.\n",
    "- Esto quiere decir que hacemos 5 entrenamientos independientes.\n",
    "- El R2 final será el promedio de las 5 R2 anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-reservation",
   "metadata": {},
   "source": [
    "### Hacemos CV a un solo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "joined-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beneficial-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, X_train, y_train, scoring = \"r2\", cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prostate-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69572881, 0.79441865, 0.53650661, 0.76273673, 0.81046456])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afraid-advocacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7199710710902316\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-harvey",
   "metadata": {},
   "source": [
    "### CV iterando por todos los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "surprised-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge r2 0.7149657353747806\n",
      "lasso r2 0.6624879490932546\n",
      "sgd r2 -2.7244111201642385e+26\n",
      "knn r2 0.48072614167286093\n",
      "gradient r2 0.8855182649224836\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring = \"r2\", cv=5)\n",
    "    print(f\"{name} r2 {np.mean(scores)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-being",
   "metadata": {},
   "source": [
    "## Summmary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-opera",
   "metadata": {},
   "source": [
    "- Dividir dataset en entrenamiento / testeo para poder sacar métricas\n",
    "- La y es nuestra variable target que es la que queremos predecir\n",
    "- Siempre limpiamos y hacemos el preprocesado de los datos, tomamos decisiones con las filas que tengan valores nulos\n",
    "- Hay diferentes modelos de regresión, que se aproximan al mismo problema con diferentes fórmulas matemáticas\n",
    "- Dos grandes problemas -> clasificación y regresión. \n",
    "- Clasificación multiclass / multilabel "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
